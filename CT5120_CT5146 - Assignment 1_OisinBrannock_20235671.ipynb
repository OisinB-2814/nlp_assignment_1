{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'indexes': [9]}\n"
     ]
    }
   ],
   "source": [
    "lines = open(\"holbrook.txt\").readlines()\n",
    "data = []\n",
    "\n",
    "# my code\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "lines_clean = [l.strip() for l in lines] # get rid of /n\n",
    "sentence = [nltk.word_tokenize(s) for s in lines_clean] # create a list entry for each sentence to work from\n",
    "\n",
    "for s in sentence:\n",
    "    o = []\n",
    "    c = [] # create empty lists to put results into later\n",
    "    i = []\n",
    "    n = 0\n",
    "    for w in s:\n",
    "        if '|' in w:\n",
    "            splt = w.split('|') # split the sentence on |\n",
    "            o.append(splt[0]) # put everything on the left of | in o\n",
    "            c.append(splt[1]) # put everything on the right of | in c\n",
    "            i.append(n) # add the index location of | to i\n",
    "        else:\n",
    "            o.append(w) # if no | present then add sentence to o unchanged\n",
    "            c.append(w) # if no | present then add sentence to c unchanged\n",
    "        n += 1 # for every word add 1 to n until | is reached\n",
    "    d = {'original': o, 'corrected': c, 'indexes': i} # put the results into the dictionary\n",
    "    data.append(d) # append the dictionary to the empty data list given for us to use\n",
    "\n",
    "assert(data[2] == {\n",
    "    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "    'indexes': [9]\n",
    "}) # works\n",
    "\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So from the question I need to be careful:\n",
    "#  I only need the corrected training sentences not the whole list of training\n",
    "#    Therefore need to split the training set further.\n",
    "#  I also see from above its the frequency of all words, meaning I need to get rid of any special chatracters that may be counted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'Royal',\n",
       " 'Enfield',\n",
       " 'come',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'like',\n",
       " 'mine',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corrected = [i['corrected'] for i in train] # take only corrected key values\n",
    "train_corrected[3] # testing line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'Royal',\n",
       " 'Enfield',\n",
       " 'come',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'like',\n",
       " 'mine']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I found a good module in nltk for removing special characters here: https://www.nltk.org/_modules/nltk/tokenize/regexp.html and also https://towardsdatascience.com/text-processing-is-coming-c13a0e2ee15c\n",
    "# So removing special characters now:\n",
    "#   I can't run it directly on the list however its designed for strings\n",
    "#     I can loop over the elements in the list and space them using .join()!\n",
    "\n",
    "train_corrected = [nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(' '.join(i)) for i in train_corrected]\n",
    "train_original = [i['original'] for i in train]\n",
    "train_original = [nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(' '.join(i)) for i in train_original]\n",
    "train_corrected[3] # checking to see if it works - yes it does as there is no full stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combo = train_corrected + train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now doing the same work on the test set for later if needed:\n",
    "test_corrected = [i['corrected'] for i in test] # take only corrected key values\n",
    "test_corrected[3] # testing line\n",
    "test_original = [i['original'] for i in test]\n",
    "test_original[3]\n",
    "test_combo = test_corrected + test_original\n",
    "test_combo = [nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(' '.join(i)) for i in test_combo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20609"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = [' '.join(w).lower() for w in train_corrected] # I want to change train corrected to a list of sentences so I can lower every character\n",
    "u = ' '.join(u) # makes the list of sentences into one giant string\n",
    "u = nltk.word_tokenize(u) # converts the giant string back into a list of single word elements\n",
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "0.004221456645155029\n"
     ]
    }
   ],
   "source": [
    "def unigram(word):\n",
    "    # Write your code here.\n",
    "    return Counter(u).get(word,0) # we only want the result of the word we input into our function not the entire dict of freq\n",
    "\n",
    "def prob(word):\n",
    "    # Write your code here.\n",
    "    # I will need the sum of the words in the corpus for the specific word\n",
    "    return Counter(u).get(word,0) / len(u) # return word frequency / total number of words \n",
    "\n",
    "# Test your code with the following\n",
    "assert(unigram(\"me\")==87) # works correctly\n",
    "\n",
    "print(unigram('me'))\n",
    "print(prob('me'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1536070558621,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HoilAmFW8PCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mind', 'mine']\n"
     ]
    }
   ],
   "source": [
    "def get_candidates(token):\n",
    "    freq = Counter(u).most_common() # https://docs.python.org/3/library/collections.html\n",
    "    unique_words = [i[0] for i in freq] # pull out words only\n",
    "    dist = [edit_distance(d, token) for d in unique_words] # get the distance list of every unique word from the token\n",
    "    dist_dict = dict(zip(unique_words,dist)) # dist_dict gives the edit distance for each word in the corpus\n",
    "    #Now I need to order them from smallest to largest - can use sorted():\n",
    "    sorted_dist_dict = {k:v for k, v in sorted(dist_dict.items(), key = lambda entry: entry[1])} # sorting by the values ascending\n",
    "    smallest_dist = next(iter(sorted_dist_dict.values())) # get the smallest entry\n",
    "    output_key = [x for x in sorted_dist_dict.keys() if sorted_dist_dict[x] == smallest_dist] # pull out the corresponding keys for the smallest distance for a word and put them into a list\n",
    "    \n",
    "    return output_key # Pulls back all words in the train_corrected list that are at smallest dist. If not at smallest dist then it just returns the word\n",
    "\n",
    "print(get_candidates(\"minde\"))\n",
    "\n",
    "# Test your code as follows\n",
    "assert get_candidates(\"minde\") == ['mind', 'mine'] # works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'white', 'cat']\n"
     ]
    }
   ],
   "source": [
    "def correct(sentence):\n",
    "    l = []\n",
    "    for word in sentence:\n",
    "        # If the word is not in the frequency distribution of words in our corpus, use get_candidates to replace the wrong word with prediction\n",
    "        if word not in set(u):\n",
    "            l.append(get_candidates(word)[0])\n",
    "        else:\n",
    "            l.append(word) # else just take the word as is and assume its correct\n",
    "    return l\n",
    "\n",
    "print(correct([\"this\",\"whitr\",\"cat\"]))\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy of words in each sentence in holbrook.txt: 0.6226056202855736\n",
      "9 sentences predicted correctly out of 100 without any error in spelling.\n"
     ]
    }
   ],
   "source": [
    "def accuracy(original, prediction): \n",
    "    # I want to get the length of the set of unique words in the prediciton that are also in the actual sentence in the corpus\n",
    "    # Then I want to divide that by the length of the actual set of words in the corpus for that sentence to get accuracy metric\n",
    "    return len(set(correct(prediction)) & set(original)) / len(set(original))\n",
    "\n",
    "accuracy = [accuracy(test_corrected[word], test_original[word]) for word in range(len(test_corrected))] # loop through the test dataset and create list of scores\n",
    "\n",
    "print(f\"Avg accuracy of words in each sentence in holbrook.txt: {sum(accuracy)/len(accuracy)}\") # return the sum of accuracies in the list divided by the length of the list to get average accuracy\n",
    "print(f\"{accuracy.count(1)} sentences predicted correctly out of 100 without any error in spelling.\") # count how many words were already fully correct by getting how many times a score of 1 appeared in the list of accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Accuracy: **62.26%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Answer: Suggestions\n",
    "\n",
    "- Going beyond unigram probability could aid in the accuracy of the model like getting bigrams probability and using them to find the likely word that should be used in the incorrect case. Could be done using interpolation (Week 4 Lectures)\n",
    "\n",
    "    - In this case we could use a back-off method for n-gram smoothing.\n",
    "\n",
    "- Using the `from nltk.corpus import words` module to check for correct spelling\n",
    "\n",
    "- Using the vocab from word.net could really help the model be better: `wn.words()`\n",
    "\n",
    "- Making sure along the way that all words are lower case when iterated over : `word.lower()`\n",
    "\n",
    "- Making sure the word is not a digit: `word is not word.isdigit()`\n",
    "\n",
    "- Using higher order n-gram smoothing back applying a the 'back-off' method\n",
    "\n",
    "- Application of distributional similarity on word vectors if a similar word cannot be found in the corpus thus increasing accuracy\n",
    "\n",
    "- Using lemmatisation and stemming (Lecture 2) \n",
    "    - Using it to identify that a word incorrectly spelled can be stemmed and returned to the correct version using the rest of the dictionaries supplied.\n",
    "        - Could do a structure to say if a word in a certain criteria then stem and analyze else use lemmatisation in the cases where we need a morphological transformation to a common lemma (e.g ies,ing,ied ended words)\n",
    "            - Finally from here we could even look for inflectional patterns\n",
    "            \n",
    "            \n",
    "- Part of speech tagging of each sentence could determine which part is incorrect and return them to us for fixing with out `get_candidates()` function\n",
    "\n",
    "- Incoroporation of even more data would be helpful like using something like Framenet, Word2Vev, GloVe, T5 or a global lexicon\n",
    "\n",
    "- Named Entity Recognition could be used to say if the word has a tag then we know its valid and if not use `get_candidates()` (Lecture 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [' '.join(w).lower() for w in train_corrected]\n",
    "p = ' '.join(u)\n",
    "data = nltk.word_tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nltk.pos_tag(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags = [x[1] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'white', 'cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words\n",
    "\n",
    "def correct_new(sentence):\n",
    "    l = []\n",
    "    for word in sentence:\n",
    "        # If the word (making sure its lower case) is not in the frequency dist of words in our corpus, use get_candidates to replace the wrong word with prediction\n",
    "        if word.lower() not in wn.words() and word.lower() not in set(u) and word.lower() is not word.isdigit() and word.lower() not in words.words() and nltk.pos_tag(word.lower()) not in data_tags:\n",
    "            l.append(get_candidates(word)[0])\n",
    "        else:\n",
    "            l.append(word) # else just take the word as is and assume its correct\n",
    "    return l\n",
    "\n",
    "print(correct_new([\"this\",\"whitr\",\"cat\"]))\n",
    "assert(correct_new([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Hw6PzwWn7iEo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy of words in each sentence: 0.8096063291140686\n",
      "17 sentences predicted correctly out of 100 without any error in spelling.\n"
     ]
    }
   ],
   "source": [
    "def accuracy_new(original, prediction): \n",
    "    # I want to get the length of the set of unique words in the prediciton that are also in the actual sentence in the corpus training data\n",
    "    # Then I want to divide that by the length of the actual set of words in the corpus for that sentence\n",
    "    return len(set(correct_new(prediction)) & set(original)) / len(set(original))\n",
    "\n",
    "accuracy_new = [accuracy_new(test_corrected[word], test_original[word]) for word in range(len(test_corrected))] # loop through the test dataset\n",
    "\n",
    "print(f\"Avg accuracy of words in each sentence: {sum(accuracy_new) / len(accuracy_new)}\") \n",
    "print(f\"{accuracy_new.count(1)} sentences predicted correctly out of 100 without any error in spelling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Accuracy: **80.96%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.700070882849495"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*((sum(accuracy_new) / len(accuracy_new)) - (sum(accuracy) / len(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.05882352941177"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(accuracy_new.count(1) - accuracy.count(1)) / accuracy_new.count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to get the model improved by **18.7%** using the proposed techniques and **47.06%** more sentences fully correctly classified. The second accuracy model takes a little longer to run due to the fact it needs to check wordnet for words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CT5120/CT5146 - Assignment 1",
   "provenance": [
    {
     "file_id": "12crGPce24mcgITZPs7hU_9CPnLAcyIq6",
     "timestamp": 1603097790764
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
